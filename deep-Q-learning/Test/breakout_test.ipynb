{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten\n",
    "from keras.optimizers import Adam, Nadam, Adamax\n",
    "sizes = (84,84,4)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.gamma = 0.99   # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.00  # exploration will not decay futher\n",
    "        self.epsilon_decay = 0.000995\n",
    "        self.learning_rate = 0.001\n",
    "        self.loss = 0\n",
    "        self.model = self._build_model()\n",
    "        self.weight_backup = 'model_weights.h5'\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=8, subsample=(4, 4), activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros', padding='same', input_shape= sizes))#80*80*4\n",
    "        model.add(Conv2D(64, kernel_size=4, subsample=(2, 2), activation='relu', padding='same'))\n",
    "        model.add(Conv2D(64, kernel_size=3, subsample=(1, 1), activation='relu', padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def save_model(self):\n",
    "            self.model.save(self.weight_backup)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            #print(act_values)\n",
    "            return np.argmax(act_values[0])\n",
    "\n",
    "    def stack(self, processed_observation, new_game):\n",
    "        global s_t, s_t1\n",
    "        if new_game:\n",
    "            s_t = np.stack((processed_observation, processed_observation, processed_observation, processed_observation), axis=2)\n",
    "            s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])\n",
    "            processed_stack = s_t\n",
    "            return processed_stack\n",
    "        else:\n",
    "            x_t1 = processed_observation.reshape(1, processed_observation.shape[0], processed_observation.shape[1], 1)\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "            processed_stack = s_t1\n",
    "            return processed_stack\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        if len(self.memory) >= 50000:\n",
    "            self.memory.popleft()\n",
    "            self.memory.append([state, action, reward, new_state, done])\n",
    "        else:\n",
    "            self.memory.append([state, action, reward, new_state, done])    \n",
    "\n",
    "    def memory_replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        Sample = random.sample(self.memory, batch_size)\n",
    "        '''\n",
    "        winsample = [s for s in self.memory if s[2]== 1.0]\n",
    "        #print(winsample)\n",
    "        tuple(winsample)\n",
    "        if len(winsample) > 1:\n",
    "            Samplewin = random.sample(winsample, 1)\n",
    "            Sample += Samplewin\n",
    "        else:  \n",
    "            Sample += winsample\n",
    "        '''\n",
    "        inputs = np.zeros((len(Sample), Sample[0][0].shape[1], Sample[0][0].shape[2], Sample[0][0].shape[3])) # minibatch input\n",
    "        targets = np.zeros((inputs.shape[0], self.action_size))\n",
    "        \n",
    "        for i in range(0, len(Sample)):\n",
    "            sample_state = Sample[i][0]\n",
    "            sample_action = Sample[i][1]\n",
    "            sample_reward = Sample[i][2]\n",
    "            sample_new_state = Sample[i][3]\n",
    "            sample_done = Sample[i][4]\n",
    "            \n",
    "#             xxx = sample_new_state.reshape(sample_new_state.shape[1], sample_new_state.shape[2])\n",
    "#             img = Image.fromarray(xxx, 'L')\n",
    "#             img.show()\n",
    "            \n",
    "            inputs[i:i+1] = sample_state # slice of inputs setting = to state\n",
    "            \n",
    "            targets[i] = self.model.predict(sample_state)\n",
    "            future_reward = self.model.predict(sample_new_state)\n",
    "            \n",
    "            if sample_done:\n",
    "                targets[i, sample_action] = sample_reward\n",
    "            #elif sample_reward == 1.0:\n",
    "                #targets[i, sample_action] = sample_reward\n",
    "            #elif sample_reward == -1.0:\n",
    "                #targets[i, sample_action] = sample_reward\n",
    "            else:\n",
    "                targets[i, sample_action] = sample_reward + self.gamma * np.max(future_reward)\n",
    "            #print(sample_action, sample_reward, targets[i, sample_action])\n",
    "            #print(targets)\n",
    "            #print(sample_action)\n",
    "        self.loss += self.model.train_on_batch(inputs, targets)\n",
    "        #print(self.loss)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "    '''        \n",
    "    def RGBprocess(new_state):\n",
    "        processed_observation = Image.fromarray(new_state, 'RGB')\n",
    "        processed_observation = processed_observation.convert('L')\n",
    "        processed_observation = processed_observation.resize((80, 80))\n",
    "        processed_observation = np.array(processed_observation)\n",
    "        processed_observation = processed_observation.reshape(1, processed_observation.shape[0], processed_observation.shape[1], 1) #1x80x80x1\n",
    "        return processed_observation\n",
    "        #stack4.append(processed_observation)\n",
    "        #if len(stack4) == 4:\n",
    "            #stack_of_observation = np.stack((processed_observation, processed_observation, processed_observation, processed_observation), axis=2)\n",
    "            #stack_of_observation = stack_of_observation.reshape(stack_of_observation.shape[0], stack_of_observation.shape[1], stack_of_observation.shape[3], stack_of_observation.shape[2])\n",
    "            #print(stack_of_observation.shape)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:48:00,184] Making new env: Breakout-v0\n",
      "[2017-08-24 16:48:00,442] Clearing 15 monitor files from previous run (because force=True was provided)\n",
      "/home/z0m6ie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, kernel_size=8, activation=\"relu\", kernel_initializer=\"random_uniform\", bias_initializer=\"zeros\", padding=\"same\", input_shape=(84, 84, 4..., strides=(4, 4))`\n",
      "/home/z0m6ie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=4, activation=\"relu\", padding=\"same\", strides=(2, 2))`\n",
      "/home/z0m6ie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=3, activation=\"relu\", padding=\"same\", strides=(1, 1))`\n",
      "[2017-08-24 16:48:00,533] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000000.mp4\n",
      "[2017-08-24 16:48:06,289] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episode, score = 1.0 \n",
      "2 episode, score = 6.0 \n",
      "3 episode, score = 1.0 \n",
      "4 episode, score = 0.0 \n",
      "5 episode, score = 2.0 \n",
      "6 episode, score = 3.0 \n",
      "7 episode, score = 1.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:48:14,783] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 episode, score = 3.0 \n",
      "9 episode, score = 1.0 \n",
      "10 episode, score = 0.0 \n",
      "11 episode, score = 4.0 \n",
      "12 episode, score = 0.0 \n",
      "13 episode, score = 0.0 \n",
      "14 episode, score = 1.0 \n",
      "15 episode, score = 3.0 \n",
      "16 episode, score = 5.0 \n",
      "17 episode, score = 3.0 \n",
      "18 episode, score = 1.0 \n",
      "19 episode, score = 1.0 \n",
      "20 episode, score = 1.0 \n",
      "21 episode, score = 2.0 \n",
      "22 episode, score = 0.0 \n",
      "23 episode, score = 2.0 \n",
      "24 episode, score = 0.0 \n",
      "25 episode, score = 3.0 \n",
      "26 episode, score = 2.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:48:35,395] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 episode, score = 2.0 \n",
      "28 episode, score = 2.0 \n",
      "29 episode, score = 3.0 \n",
      "30 episode, score = 0.0 \n",
      "31 episode, score = 0.0 \n",
      "32 episode, score = 1.0 \n",
      "33 episode, score = 0.0 \n",
      "34 episode, score = 1.0 \n",
      "35 episode, score = 1.0 \n",
      "36 episode, score = 2.0 \n",
      "37 episode, score = 6.0 \n",
      "38 episode, score = 0.0 \n",
      "39 episode, score = 2.0 \n",
      "40 episode, score = 2.0 \n",
      "41 episode, score = 1.0 \n",
      "42 episode, score = 1.0 \n",
      "43 episode, score = 3.0 \n",
      "44 episode, score = 1.0 \n",
      "45 episode, score = 2.0 \n",
      "46 episode, score = 2.0 \n",
      "47 episode, score = 1.0 \n",
      "48 episode, score = 1.0 \n",
      "49 episode, score = 0.0 \n",
      "50 episode, score = 1.0 \n",
      "51 episode, score = 3.0 \n",
      "52 episode, score = 2.0 \n",
      "53 episode, score = 0.0 \n",
      "54 episode, score = 1.0 \n",
      "55 episode, score = 0.0 \n",
      "56 episode, score = 2.0 \n",
      "57 episode, score = 1.0 \n",
      "58 episode, score = 3.0 \n",
      "59 episode, score = 4.0 \n",
      "60 episode, score = 1.0 \n",
      "61 episode, score = 2.0 \n",
      "62 episode, score = 1.0 \n",
      "63 episode, score = 2.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:49:20,843] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 episode, score = 2.0 \n",
      "65 episode, score = 0.0 \n",
      "66 episode, score = 2.0 \n",
      "67 episode, score = 3.0 \n",
      "68 episode, score = 1.0 \n",
      "69 episode, score = 2.0 \n",
      "70 episode, score = 2.0 \n",
      "71 episode, score = 0.0 \n",
      "72 episode, score = 0.0 \n",
      "73 episode, score = 0.0 \n",
      "74 episode, score = 0.0 \n",
      "75 episode, score = 0.0 \n",
      "76 episode, score = 2.0 \n",
      "77 episode, score = 2.0 \n",
      "78 episode, score = 2.0 \n",
      "79 episode, score = 4.0 \n",
      "80 episode, score = 1.0 \n",
      "81 episode, score = 0.0 \n",
      "82 episode, score = 0.0 \n",
      "83 episode, score = 4.0 \n",
      "84 episode, score = 1.0 \n",
      "85 episode, score = 0.0 \n",
      "86 episode, score = 0.0 \n",
      "87 episode, score = 3.0 \n",
      "88 episode, score = 0.0 \n",
      "89 episode, score = 2.0 \n",
      "90 episode, score = 2.0 \n",
      "91 episode, score = 0.0 \n",
      "92 episode, score = 2.0 \n",
      "93 episode, score = 4.0 \n",
      "94 episode, score = 0.0 \n",
      "95 episode, score = 5.0 \n",
      "96 episode, score = 3.0 \n",
      "97 episode, score = 2.0 \n",
      "98 episode, score = 0.0 \n",
      "99 episode, score = 1.0 \n",
      "100 episode, score = 0.0 \n",
      "101 episode, score = 1.0 \n",
      "102 episode, score = 1.0 \n",
      "103 episode, score = 2.0 \n",
      "104 episode, score = 1.0 \n",
      "105 episode, score = 6.0 \n",
      "106 episode, score = 2.0 \n",
      "107 episode, score = 0.0 \n",
      "108 episode, score = 2.0 \n",
      "109 episode, score = 3.0 \n",
      "110 episode, score = 2.0 \n",
      "111 episode, score = 1.0 \n",
      "112 episode, score = 1.0 \n",
      "113 episode, score = 3.0 \n",
      "114 episode, score = 1.0 \n",
      "115 episode, score = 0.0 \n",
      "116 episode, score = 0.0 \n",
      "117 episode, score = 3.0 \n",
      "118 episode, score = 0.0 \n",
      "119 episode, score = 2.0 \n",
      "120 episode, score = 3.0 \n",
      "121 episode, score = 0.0 \n",
      "122 episode, score = 1.0 \n",
      "123 episode, score = 0.0 \n",
      "124 episode, score = 3.0 \n",
      "125 episode, score = 4.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:50:48,636] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 episode, score = 0.0 \n",
      "127 episode, score = 1.0 \n",
      "128 episode, score = 0.0 \n",
      "129 episode, score = 0.0 \n",
      "130 episode, score = 2.0 \n",
      "131 episode, score = 0.0 \n",
      "132 episode, score = 0.0 \n",
      "133 episode, score = 2.0 \n",
      "134 episode, score = 1.0 \n",
      "135 episode, score = 1.0 \n",
      "136 episode, score = 1.0 \n",
      "137 episode, score = 3.0 \n",
      "138 episode, score = 0.0 \n",
      "139 episode, score = 2.0 \n",
      "140 episode, score = 0.0 \n",
      "141 episode, score = 1.0 \n",
      "142 episode, score = 2.0 \n",
      "143 episode, score = 0.0 \n",
      "144 episode, score = 2.0 \n",
      "145 episode, score = 2.0 \n",
      "146 episode, score = 0.0 \n",
      "147 episode, score = 1.0 \n",
      "148 episode, score = 3.0 \n",
      "149 episode, score = 2.0 \n",
      "150 episode, score = 0.0 \n",
      "151 episode, score = 3.0 \n",
      "152 episode, score = 1.0 \n",
      "153 episode, score = 0.0 \n",
      "154 episode, score = 1.0 \n",
      "155 episode, score = 0.0 \n",
      "156 episode, score = 0.0 \n",
      "157 episode, score = 2.0 \n",
      "158 episode, score = 2.0 \n",
      "159 episode, score = 2.0 \n",
      "160 episode, score = 3.0 \n",
      "161 episode, score = 3.0 \n",
      "162 episode, score = 0.0 \n",
      "163 episode, score = 0.0 \n",
      "164 episode, score = 2.0 \n",
      "165 episode, score = 2.0 \n",
      "166 episode, score = 8.0 \n",
      "167 episode, score = 1.0 \n",
      "168 episode, score = 1.0 \n",
      "169 episode, score = 0.0 \n",
      "170 episode, score = 4.0 \n",
      "171 episode, score = 4.0 \n",
      "172 episode, score = 3.0 \n",
      "173 episode, score = 1.0 \n",
      "174 episode, score = 3.0 \n",
      "175 episode, score = 3.0 \n",
      "176 episode, score = 0.0 \n",
      "177 episode, score = 0.0 \n",
      "178 episode, score = 0.0 \n",
      "179 episode, score = 2.0 \n",
      "180 episode, score = 3.0 \n",
      "181 episode, score = 0.0 \n",
      "182 episode, score = 0.0 \n",
      "183 episode, score = 1.0 \n",
      "184 episode, score = 0.0 \n",
      "185 episode, score = 0.0 \n",
      "186 episode, score = 3.0 \n",
      "187 episode, score = 1.0 \n",
      "188 episode, score = 2.0 \n",
      "189 episode, score = 0.0 \n",
      "190 episode, score = 2.0 \n",
      "191 episode, score = 1.0 \n",
      "192 episode, score = 1.0 \n",
      "193 episode, score = 3.0 \n",
      "194 episode, score = 0.0 \n",
      "195 episode, score = 0.0 \n",
      "196 episode, score = 4.0 \n",
      "197 episode, score = 2.0 \n",
      "198 episode, score = 4.0 \n",
      "199 episode, score = 0.0 \n",
      "200 episode, score = 3.0 \n",
      "201 episode, score = 2.0 \n",
      "202 episode, score = 1.0 \n",
      "203 episode, score = 1.0 \n",
      "204 episode, score = 3.0 \n",
      "205 episode, score = 0.0 \n",
      "206 episode, score = 0.0 \n",
      "207 episode, score = 1.0 \n",
      "208 episode, score = 0.0 \n",
      "209 episode, score = 0.0 \n",
      "210 episode, score = 2.0 \n",
      "211 episode, score = 0.0 \n",
      "212 episode, score = 1.0 \n",
      "213 episode, score = 1.0 \n",
      "214 episode, score = 1.0 \n",
      "215 episode, score = 3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-24 16:53:56,059] Starting new video recorder writing to /home/z0m6ie/Documents/GitHub/Machine_Learning_Projects/deep-Q-learning/Test/Breakout-v0/openaigym.video.0.27707.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 episode, score = 2.0 \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-674dbe87a995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtotalreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mnew_game\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\u001b[0m in \u001b[0;36m_reset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/site-packages/gym/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting ffmpeg with \"%s\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'setsid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#setsid not present on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    705\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/z0m6ie/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1258\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1261\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from scipy import misc\n",
    "import skimage as skimage\n",
    "from skimage import color\n",
    "from skimage import transform\n",
    "from skimage import util\n",
    "from skimage import exposure\n",
    "#from skimage.viewer import ImageViewer\n",
    "#import cv2\n",
    "\n",
    "'''\n",
    "def RGBprocess(raw_img):\n",
    "    processed_observation = Image.fromarray(raw_img, 'RGB')\n",
    "    processed_observation = processed_observation.convert('L')\n",
    "    processed_observation = processed_observation.resize((84, 84))\n",
    "    processed_observation = np.array(processed_observation)\n",
    "    processed_observation = processed_observation.reshape(1, processed_observation.shape[0], processed_observation.shape[1], 1) #1x80x80x1\n",
    "    return processed_observation\n",
    "'''\n",
    "def RGBprocess(raw_img):\n",
    "    x_t = skimage.color.rgb2gray(raw_img) # Grayscale\n",
    "    x_t = skimage.transform.resize(x_t, (110,84), mode='constant', preserve_range=True) # Downsample\n",
    "    processed_observation = skimage.util.crop(x_t,((19,7),(0,0))) # Crop\n",
    "    return processed_observation\n",
    "'''\n",
    "def RGBprocess(raw_img): \n",
    "        I = raw_img[35:195]\n",
    "        I = I[::2, ::2, 0]\n",
    "        I[I == 144] = 0\n",
    "        I[I == 109] = 0\n",
    "        I[I != 0] = 1\n",
    "        processed_observation = I.astype(np.float32)\n",
    "        return processed_observation\n",
    "\n",
    "def RGBprocess(raw_img):\n",
    "    grayscale_observation = raw_img.mean(2)\n",
    "    resized_observation = misc.imresize(grayscale_observation, (80, 80)).astype(np.float32)\n",
    "    processed_observation = resized_observation.reshape(1, resized_observation.shape[0], resized_observation.shape[1], 1)\n",
    "    return processed_observation\n",
    "\n",
    "\n",
    "def RGBprocess(raw_img):\n",
    "    frame = cv2.cvtColor(cv2.resize(frame, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, frame = cv2.threshold(frame, 1, 255, cv2.THRESH_BINARY)\n",
    "    processed_observation = np.reshape(frame, (1, 84, 84, 1))\n",
    "    return processed_observation\n",
    "'''\n",
    "\n",
    "batch_size = 32\n",
    "#episodes = sys.argv[1] if len(sys.argv) > 1 else 5000\n",
    "#env_name = sys.argv[2] if len(sys.argv) > 2 else \"Pong-v0\"\n",
    "\n",
    "episodes = 1500\n",
    "env_name = \"Breakout-v0\"\n",
    "D = 84*84\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env = wrappers.Monitor(env, env_name, force=True)\n",
    "\n",
    "agent = Agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "for i_episodes in range(episodes):\n",
    "    State = env.reset()\n",
    "    totalreward = 0\n",
    "    new_game = True\n",
    "    prev_x = None\n",
    "    state = RGBprocess(State)\n",
    "    state = agent.stack(state, new_game)\n",
    "    short_mem = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if i_episodes % 50 == 0:\n",
    "            env.render()\n",
    "        action = agent.act(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        new_state = RGBprocess(new_state)\n",
    "        #new_state_dif = new_state - prev_x if prev_x is not None else np.zeros((1, 84, 84, 1))\n",
    "        #prev_x = new_state\n",
    "        new_state_dif = agent.stack(new_state, new_game)\n",
    "        agent.remember(state, action, reward, new_state_dif, done)\n",
    "        state = new_state_dif\n",
    "        totalreward += reward\n",
    "        new_game = False\n",
    "    agent.memory_replay(batch_size)\n",
    "    if done:\n",
    "        print(\"{} episode, score = {} \".format(i_episodes + 1, totalreward))\n",
    "        agent.save_model()\n",
    "\n",
    "env.close()\n",
    "gym.upload(env_name, api_key='sk_WRCITkqmTJKYB9hvBk5tPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "gym.upload(env_name, api_key='sk_WRCITkqmTJKYB9hvBk5tPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = agent.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_observation = Image.fromarray(state, 'RGB')\n",
    "processed_observation = processed_observation.convert('L')\n",
    "processed_observation = processed_observation.resize((80, 80))\n",
    "processed_observation = np.array(processed_observation)\n",
    "processed_observation = processed_observation.reshape(processed_observation.shape[0], processed_observation.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RGBprocess(new_state):\n",
    "        processed_observation = Image.fromarray(new_state, 'RGB')\n",
    "        processed_observation = processed_observation.convert('L')\n",
    "        processed_observation = processed_observation.resize((80, 80))\n",
    "        processed_observation = np.array(processed_observation)\n",
    "        processed_observation = processed_observation.reshape(processed_observation.shape[0], processed_observation.shape[1], 1) #1x80x80x1\n",
    "        return processed_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_state_dif = new_state_dif.reshape(new_state_dif.shape[1], new_state_dif.shape[2])\n",
    "img = Image.fromarray(new_state_dif, 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#img = Image.fromarray(state, 'L')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thing = [1,2,3,4,5,6]\n",
    "for x in thing[-2:]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # way to adjust reward, think incorrect\n",
    "        if reward == 0:\n",
    "            num_actions += 1\n",
    "            short_mem.append([state, action, reward, new_state_dif, done])\n",
    "        else:\n",
    "            num_actions += 1\n",
    "            short_mem.append([state, action, reward, new_state_dif, done])\n",
    "            if reward == -1.0:\n",
    "                for m in short_mem:\n",
    "                    m[2] = -1.0\n",
    "                    agent.remember(m[0], m[1], m[2], m[3], m[4])\n",
    "                num_actions = 0\n",
    "            elif reward == 1.0:\n",
    "                for m in short_mem:\n",
    "                    m[2] = 1.0\n",
    "                    agent.remember(m[0], m[1], m[2], m[3], m[4])\n",
    "            short_mem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # early attempt at increasing samples with positive reward\n",
    "        winsample = [s for s in self.memory if s[2]== 1.0]\n",
    "        #print(winsample)\n",
    "        tuple(winsample)\n",
    "        if len(winsample) > 4:\n",
    "            Samplewin = random.sample(winsample, 4)\n",
    "            Sample += Samplewin\n",
    "        else:  \n",
    "            Sample += winsample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
